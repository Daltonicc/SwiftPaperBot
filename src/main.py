"""
ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
Swift & iOS ë…¼ë¬¸ ìš”ì•½ ìŠ¬ë™ë´‡ì˜ ë©”ì¸ ì‹¤í–‰ íŒŒì¼ì…ë‹ˆë‹¤.
"""
import logging
import sys
from typing import List, Tuple
from datetime import datetime

from .config import Config
from .arxiv_client import ArxivClient, Paper
from .summarizer import PaperSummarizer, PaperSummary
from .slack_client import SlackClient
from .database import DatabaseManager
from .scheduler import TaskScheduler

logger = logging.getLogger(__name__)

class SwiftPaperBot:
    """Swift ë…¼ë¬¸ ìš”ì•½ ìŠ¬ë™ë´‡ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ì„¤ì • ê²€ì¦
        if not Config.validate_config():
            raise ValueError("í•„ìˆ˜ ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. config.env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.arxiv_client = ArxivClient()
        self.summarizer = PaperSummarizer(Config.OPENAI_API_KEY or "")
        self.slack_client = SlackClient(Config.SLACK_BOT_TOKEN or "", Config.SLACK_CHANNEL)
        self.db_manager = DatabaseManager(Config.DATABASE_PATH)
        self.scheduler = TaskScheduler()
        
        logger.info("SwiftPaperBot ì´ˆê¸°í™” ì™„ë£Œ")
    
    def daily_paper_summary_task(self):
        """ë§¤ì¼ ì‹¤í–‰ë˜ëŠ” ë…¼ë¬¸ ìš”ì•½ ì‘ì—… (í™•ì¥ëœ ê¸°ëŠ¥)"""
        logger.info("=== ì¼ì¼ ë…¼ë¬¸ ìš”ì•½ ì‘ì—… ì‹œì‘ ===")
        
        try:
            # 1. arXivì—ì„œ ìƒˆë¡œìš´ ë…¼ë¬¸ ê²€ìƒ‰
            logger.info(f"arXivì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘... (ìµœê·¼ {Config.ARXIV_SEARCH_DAYS}ì¼)")
            papers = self.arxiv_client.search_papers(days_back=Config.ARXIV_SEARCH_DAYS)
            
            if not papers:
                logger.info("ìƒˆë¡œìš´ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤")
                # ë¹ˆ ë©”ì‹œì§€ì™€ í•¨ê»˜ ê¸°ë³¸ í†µê³„ ì „ì†¡
                stats = self.db_manager.get_statistics(days=7)
                self.slack_client.send_paper_summaries([], [], stats)
                return
            
            logger.info(f"{len(papers)}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤")
            
            # 2. ë…¼ë¬¸ í•„í„°ë§ ë° í™•ì¥ëœ ìš”ì•½ ìƒì„±
            candidate_summaries = []
            candidate_papers = []
            all_summaries = []  # í†µê³„ìš©
            
            for paper in papers:
                # ì´ë¯¸ ì „ì†¡ëœ ë…¼ë¬¸ì¸ì§€ í™•ì¸
                if self.db_manager.is_paper_sent_today(paper.id):
                    logger.info(f"ì´ë¯¸ ì „ì†¡ëœ ë…¼ë¬¸ ê±´ë„ˆë›°ê¸°: {paper.title}")
                    continue
                
                # ë…¼ë¬¸ ì •ë³´ ì €ì¥
                self.db_manager.save_paper(paper)
                
                # ë…¼ë¬¸ ìš”ì•½ ìƒì„± (í™•ì¥ëœ ê¸°ëŠ¥)
                logger.info(f"ë…¼ë¬¸ ìš”ì•½ ìƒì„± ì¤‘: {paper.title}")
                summary = self.summarizer.summarize_paper(paper)
                
                if summary:
                    # ìš”ì•½ ì €ì¥
                    self.db_manager.save_summary(summary)
                    all_summaries.append(summary)
                    
                    # ê´€ë ¨ì„± ì²´í¬
                    if self.summarizer.is_relevant_paper(summary, min_score=Config.MIN_RELEVANCE_SCORE):
                        candidate_summaries.append(summary)
                        candidate_papers.append(paper)
                        logger.info(f"ê´€ë ¨ì„± ìˆëŠ” ë…¼ë¬¸ ë°œê²¬: {paper.title} (ì ìˆ˜: {summary.relevance_score})")
                    else:
                        logger.info(f"ê´€ë ¨ì„± ë‚®ì€ ë…¼ë¬¸ ì œì™¸: {paper.title} (ì ìˆ˜: {summary.relevance_score})")
                else:
                    logger.warning(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {paper.title}")
            
            # 3. í†µê³„ ìƒì„±
            stats = self._generate_comprehensive_stats(all_summaries)
            
            # 4. ê´€ë ¨ì„± ì ìˆ˜ì™€ ìµœì‹ ì„± ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ë…¼ë¬¸ ì„ íƒ
            if candidate_summaries:
                # ê´€ë ¨ì„± ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ, ê²Œì‹œì¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_indices = sorted(
                    range(len(candidate_summaries)),
                    key=lambda i: (candidate_summaries[i].relevance_score, candidate_papers[i].published),
                    reverse=True
                )
                
                # ìƒìœ„ ë…¼ë¬¸ë“¤ë§Œ ì„ íƒ
                max_papers = min(Config.MAX_DAILY_PAPERS, len(sorted_indices))
                top_summaries = [candidate_summaries[i] for i in sorted_indices[:max_papers]]
                top_papers = [candidate_papers[i] for i in sorted_indices[:max_papers]]
                
                logger.info(f"ìƒìœ„ {len(top_summaries)}ê°œ ë…¼ë¬¸ ì„ íƒ (ì´ {len(candidate_summaries)}ê°œ ì¤‘)")
                for i, summary in enumerate(top_summaries, 1):
                    paper = top_papers[i-1]
                    logger.info(f"  {i}. {paper.title} (ê´€ë ¨ì„±: {summary.relevance_score}/10, ë‚ ì§œ: {paper.published.date()})")
                
                # 5. ìŠ¬ë™ìœ¼ë¡œ í™•ì¥ëœ ìš”ì•½ ì „ì†¡
                success = self.slack_client.send_paper_summaries(top_summaries, top_papers, stats)
                
                if success:
                    # ì „ì†¡ ì„±ê³µ ì‹œ ì „ì†¡ ê¸°ë¡ ì €ì¥
                    for paper in top_papers:
                        self.db_manager.mark_paper_as_sent(paper.id)
                    
                    # ì¼ì¼ í†µê³„ ì €ì¥
                    today = datetime.now().strftime("%Y-%m-%d")
                    self.db_manager.save_daily_statistics(today, stats)
                    
                    logger.info("ìŠ¬ë™ ì „ì†¡ ë° ê¸°ë¡ ì €ì¥ ì™„ë£Œ")
                else:
                    logger.error("ìŠ¬ë™ ì „ì†¡ ì‹¤íŒ¨")
                    self.slack_client.send_error_notification("ë…¼ë¬¸ ìš”ì•½ ì „ì†¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                logger.info(f"ê´€ë ¨ì„± ë†’ì€ ë…¼ë¬¸ì´ ì—†ì–´ ë¹ˆ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤ ({Config.MIN_RELEVANCE_SCORE}ì  ì´ìƒ ê¸°ì¤€)")
                self.slack_client.send_paper_summaries([], [], stats)
            
            # 6. ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ (ì£¼ 1íšŒ)
            if datetime.now().weekday() == 0:  # ì›”ìš”ì¼
                logger.info("ì£¼ê°„ ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ì‹¤í–‰")
                self.db_manager.cleanup_old_data(days=30)
            
        except Exception as e:
            logger.error(f"ì¼ì¼ ë…¼ë¬¸ ìš”ì•½ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.slack_client.send_error_notification(f"ë…¼ë¬¸ ìš”ì•½ ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        logger.info("=== ì¼ì¼ ë…¼ë¬¸ ìš”ì•½ ì‘ì—… ì™„ë£Œ ===")
    
    def _generate_comprehensive_stats(self, summaries: List[PaperSummary]) -> dict:
        """ì¢…í•©ì ì¸ í†µê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
        try:
            # ê¸°ë³¸ í†µê³„
            basic_stats = self.summarizer.get_summary_statistics(summaries)
            
            # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ (ìµœê·¼ 30ì¼)
            db_stats = self.db_manager.get_statistics(days=30)
            
            # í†µí•© í†µê³„
            combined_stats = {
                **basic_stats,
                'db_total_papers': db_stats.get('total_papers', 0),
                'db_avg_relevance': db_stats.get('avg_relevance_score', 0),
                'db_category_distribution': db_stats.get('category_distribution', {}),
                'daily_breakdown': db_stats.get('daily_breakdown', []),
                'top_keywords': db_stats.get('top_keywords', {}),
                'relevance_rate': db_stats.get('relevance_rate', 0)
            }
            
            return combined_stats
            
        except Exception as e:
            logger.error(f"í†µê³„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {}
    
    def start_scheduler(self, schedule_time: str = "08:00"):
        """ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤"""
        logger.info(f"ìŠ¬ë™ë´‡ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ - ë§¤ì¼ {schedule_time}")
        
        # ìŠ¬ë™ ì—°ê²° í…ŒìŠ¤íŠ¸
        if not self.slack_client.send_test_message():
            raise ConnectionError("ìŠ¬ë™ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í† í°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # ìŠ¤ì¼€ì¤„ ë“±ë¡
        self.scheduler.schedule_daily_task(
            self.daily_paper_summary_task,
            schedule_time
        )
        
        # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ë¡œê·¸
        next_run = self.scheduler.get_next_run_time()
        logger.info(f"ë‹¤ìŒ ì‹¤í–‰ ì˜ˆì •: {next_run}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰
        self.scheduler.run_scheduler()
    
    def run_once(self):
        """í•œ ë²ˆë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸ìš©)"""
        logger.info("ë…¼ë¬¸ ìš”ì•½ ì‘ì—…ì„ í•œ ë²ˆ ì‹¤í–‰í•©ë‹ˆë‹¤")
        self.daily_paper_summary_task()
    
    def get_statistics(self):
        """ë´‡ í†µê³„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤ (í™•ì¥ëœ í†µê³„)"""
        stats = self.db_manager.get_statistics(days=30)
        
        logger.info("=== ë´‡ í†µê³„ (ìµœê·¼ 30ì¼) ===")
        logger.info(f"ì´ ì €ì¥ëœ ë…¼ë¬¸ ìˆ˜: {stats.get('total_papers', 0)}")
        logger.info(f"í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: {stats.get('avg_relevance_score', 0)}/10")
        logger.info(f"ë†’ì€ ê´€ë ¨ì„± ë…¼ë¬¸ ìˆ˜: {stats.get('high_relevance_count', 0)}")
        logger.info(f"ê´€ë ¨ì„± ë¹„ìœ¨: {stats.get('relevance_rate', 0)}%")
        
        # ì¹´í…Œê³ ë¦¬ ë¶„í¬
        if stats.get('category_distribution'):
            logger.info("ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
            for category, count in stats['category_distribution'].items():
                logger.info(f"  - {category}: {count}ê°œ")
        
        # ìƒìœ„ í‚¤ì›Œë“œ
        if stats.get('top_keywords'):
            logger.info("ìƒìœ„ í‚¤ì›Œë“œ:")
            for keyword, freq in list(stats['top_keywords'].items())[:5]:
                logger.info(f"  - {keyword}: {freq}íšŒ")
        
        return stats
    
    def test_enhanced_features(self):
        """í™•ì¥ëœ ê¸°ëŠ¥ë“¤ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤"""
        logger.info("=== í™•ì¥ëœ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        
        try:
            # 1. ìµœê·¼ ë…¼ë¬¸ 1ê°œ ê°€ì ¸ì™€ì„œ í™•ì¥ëœ ìš”ì•½ í…ŒìŠ¤íŠ¸
            papers = self.arxiv_client.search_papers(days_back=7)
            if papers:
                test_paper = papers[0]
                logger.info(f"í…ŒìŠ¤íŠ¸ ë…¼ë¬¸: {test_paper.title}")
                
                # í™•ì¥ëœ ìš”ì•½ ìƒì„±
                summary = self.summarizer.summarize_paper(test_paper)
                if summary:
                    logger.info("=== í™•ì¥ëœ ìš”ì•½ ê²°ê³¼ ===")
                    logger.info(f"ê´€ë ¨ì„± ì ìˆ˜: {summary.relevance_score}/10")
                    logger.info(f"Swift í‚¤ì›Œë“œ ì ìˆ˜: {summary.swift_keywords_score}/10")
                    logger.info(f"ì˜ˆì¸¡ ì¹´í…Œê³ ë¦¬: {summary.category_prediction}")
                    logger.info(f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: {summary.extracted_keywords[:5]}")
                    logger.info(f"ê¸°ìˆ ì  ìš”ì•½: {summary.technical_summary[:100]}...")
                    logger.info(f"ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸: {summary.business_impact[:100]}...")
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    self.db_manager.save_paper(test_paper)
                    self.db_manager.save_summary(summary)
                    
                    logger.info("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
                else:
                    logger.error("ìš”ì•½ ìƒì„± ì‹¤íŒ¨")
            
            # 2. í†µê³„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            stats = self.db_manager.get_statistics(days=7)
            logger.info("=== í†µê³„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
            logger.info(f"ìµœê·¼ 7ì¼ í†µê³„: {stats}")
            
            logger.info("=== í™•ì¥ëœ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
            
        except Exception as e:
            logger.error(f"í™•ì¥ëœ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    Config.setup_logging()
    
    logger.info("Swift ë…¼ë¬¸ ìš”ì•½ ìŠ¬ë™ë´‡ ì‹œì‘")
    
    # ì„¤ì • ì •ë³´ ì¶œë ¥
    print("âœ… ëª¨ë“  í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“Š ê²€ìƒ‰ ì„¤ì •: ìµœëŒ€ {Config.ARXIV_MAX_RESULTS}ê°œ ë…¼ë¬¸, ìµœê·¼ {Config.ARXIV_SEARCH_DAYS}ì¼")
    print(f"ğŸ” í•„í„°ë§: ê´€ë ¨ì„± {Config.MIN_RELEVANCE_SCORE}ì  ì´ìƒ, ì¼ì¼ ìµœëŒ€ {Config.MAX_DAILY_PAPERS}í¸")
    
    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶œë ¥
    if hasattr(Config, 'ARXIV_SEARCH_TERMS') and Config.ARXIV_SEARCH_TERMS:
        terms = str(Config.ARXIV_SEARCH_TERMS).split(',')
        terms_preview = [term.strip().strip('"') for term in terms[:4]]
        print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(terms_preview)}... (ë“± ì´ {len(terms)}ê°œ)")
    
    try:
        bot = SwiftPaperBot()
        
        # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
        if len(sys.argv) > 1:
            command = sys.argv[1].lower()
            
            if command == "once":
                # í•œ ë²ˆë§Œ ì‹¤í–‰
                bot.run_once()
            elif command == "stats":
                # í†µê³„ ì¶œë ¥
                bot.get_statistics()
            elif command == "test":
                # í™•ì¥ëœ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
                bot.test_enhanced_features()
            elif command.startswith("schedule"):
                # ìŠ¤ì¼€ì¤„ ì‹¤í–‰ (ì‹œê°„ ì§€ì • ê°€ëŠ¥)
                time_str = "08:00"
                if len(sys.argv) > 2:
                    time_str = sys.argv[2]
                bot.start_scheduler(time_str)
            else:
                print("ì‚¬ìš©ë²•:")
                print("  python -m src.main once        # í•œ ë²ˆë§Œ ì‹¤í–‰")
                print("  python -m src.main stats       # í†µê³„ ì¶œë ¥")
                print("  python -m src.main test        # í™•ì¥ëœ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
                print("  python -m src.main schedule [HH:MM]  # ìŠ¤ì¼€ì¤„ ì‹¤í–‰")
        else:
            # ê¸°ë³¸ê°’: ì˜¤ì „ 8ì‹œ ìŠ¤ì¼€ì¤„ ì‹¤í–‰
            bot.start_scheduler("08:00")
            
    except Exception as e:
        logger.error(f"ë´‡ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()